<?xml version="1.0" encoding="UTF-8" ?>

<!--  
 This is the Solr schema file. This file should be named "schema.xml" and
 should be in the conf directory under the solr home
 (i.e. ./solr/conf/schema.xml by default) 
 or located where the classloader for the Solr webapp can find it.

 This example schema is the recommended starting point for users.
 It should be kept correct and concise, usable out-of-the-box.

 For more information, on how to customize this file, please see
 http://wiki.apache.org/solr/SchemaXml

 PERFORMANCE NOTE: this schema includes many optional features and should not
 be used for benchmarking.  To improve performance one could
  - set stored="false" for all fields possible (esp large fields) when you
    only need to search on the field but don't need to return the original
    value.
  - set indexed="false" if you don't need to search on the field, but only
    return the field as a result of searching on other indexed fields.
  - remove all unneeded copyField statements
  - for best index size and searching performance, set "index" to false
    for all general text fields, use copyField to copy them to the
    catchall "text" field, and use that for searching.
  - For maximum indexing performance, use the StreamingUpdateSolrServer
    java client.
  - Remember to run the JVM in server mode, and use a higher logging level
    that avoids logging every request
-->

<schema name="example" version="1.3">

  <types>

    <!-- The StrField type is not analyzed, but indexed/stored verbatim. -->
    <fieldType name="string" class="solr.StrField" sortMissingLast="true" omitNorms="true"/>

    <!-- boolean type: "true" or "false" -->
    <fieldType name="boolean" class="solr.BoolField" sortMissingLast="true" omitNorms="true"/>
    <!--Binary data type. The data should be sent/retrieved in as Base64 encoded Strings -->
    <fieldtype name="binary" class="solr.BinaryField"/>

    <!--
      Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types.
    -->
    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>

    <!--
     Numeric field types that index each value at various levels of precision
     to accelerate range queries when the number of values between the range
     endpoints is large. See the javadoc for NumericRangeQuery for internal
     implementation details.

     Smaller precisionStep values (specified in bits) will lead to more tokens
     indexed per value, slightly larger index size, and faster range queries.
     A precisionStep of 0 disables indexing at different precision levels.
    -->
    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>

    <!-- The format for this date field is of the form 1995-12-31T23:59:59Z, and
         is a more restricted form of the canonical representation of dateTime
         http://www.w3.org/TR/xmlschema-2/#dateTime    
         The trailing "Z" designates UTC time and is mandatory.
         Optional fractional seconds are allowed: 1995-12-31T23:59:59.999Z
         All other components are mandatory.

         Expressions can also be used to denote calculations that should be
         performed relative to "NOW" to determine the value, ie...

               NOW/HOUR
                  ... Round to the start of the current hour
               NOW-1DAY
                  ... Exactly 1 day prior to now
               NOW/DAY+6MONTHS+3DAYS
                  ... 6 months and 3 days in the future from the start of
                      the current day
                      
         Consult the DateField javadocs for more information.

         Note: For faster range queries, consider the tdate type
      -->
    <fieldType name="date" class="solr.TrieDateField" omitNorms="true" precisionStep="0" positionIncrementGap="0"/>

    <!-- A Trie based date field for faster date range queries and date faceting. -->
    <fieldType name="tdate" class="solr.TrieDateField" omitNorms="true" precisionStep="6" positionIncrementGap="0"/>

    <!-- A text field that uses WordDelimiterFilter to enable splitting and matching of
        words on case-change, alpha numeric boundaries, and non-alphanumeric chars,
        so that a query of "wifi" or "wi fi" could match a document containing 
        "Wi-Fi" or "WiFi".
        Stopwords is customized by external files. Stemming is disabled.
    -->
    <fieldType name="text" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">

        <tokenizer class="solr.StandardTokenizerFactory"/>
        
        <filter class="solr.StandardFilterFactory"/>
      
        <!-- Splits words into subwords and performs optional transformations on 
             subword groups.
             - preserveOriginal="1" in order to preserve the original word.
        -->
        <filter class="solr.WordDelimiterFilterFactory" 
                generateWordParts="1" 
                generateNumberParts="0" 
                catenateWords="0" 
                catenateNumbers="0" 
                catenateAll="0" 
                splitOnCaseChange="1" 
                preserveOriginal="1"/>
        
        <!-- Change to lowercase text -->
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <!-- Case insensitive stop word removal.
          add enablePositionIncrements=true in both the index and query
          analyzers to leave a 'gap' for more accurate phrase queries.
        -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
                
        <!-- Filters out those tokens *not* having length min through max 
             inclusive. -->
        <filter class="solr.LengthFilterFactory" min="2" max="256"/>
        
      </analyzer>
      <analyzer type="query">
        
        <tokenizer class="solr.StandardTokenizerFactory"/>
        
        <filter class="solr.StandardFilterFactory"/>
        
        <!-- Splits words into subwords and performs optional transformations on 
             subword groups.
             - preserveOriginal="1" in order to preserve the original word 
        -->
        <filter class="solr.WordDelimiterFilterFactory" 
                generateWordParts="1" 
                generateNumberParts="0" 
                catenateWords="0" 
                catenateNumbers="0" 
                catenateAll="0" 
                splitOnCaseChange="1" 
                preserveOriginal="1"/>
        
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
                
        <!-- Filters out those tokens *not* having length min through max 
             inclusive. -->
        <filter class="solr.LengthFilterFactory" min="2" max="256"/>
        
      </analyzer>
    </fieldType>
    
    <!-- A uri field that uses WhitespaceTokenizer and WordDelimiterFilter to 
         split URIs into multiple compoenents.  Stopwords is customized by 
         external files.
    -->
    <fieldType name="uri" class="solr.TextField" positionIncrementGap="100">
      <analyzer type="index">

        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
      
        <!-- Splits words into subwords and performs optional transformations on 
             subword groups.
             - preserveOriginal="1" in order to preserve the original uri.
        -->
        <filter class="solr.WordDelimiterFilterFactory" 
                generateWordParts="1" 
                generateNumberParts="0" 
                catenateWords="0" 
                catenateNumbers="0" 
                catenateAll="0" 
                splitOnCaseChange="1" 
                preserveOriginal="1"/>
        
        <!-- Change to lowercase text -->
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <!-- Case insensitive stop word removal.
          add enablePositionIncrements=true in both the index and query
          analyzers to leave a 'gap' for more accurate phrase queries.
        -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
                
        <!-- Filters out those tokens *not* having length min through max 
             inclusive. -->
        <filter class="solr.LengthFilterFactory" min="2" max="256"/>
        
      </analyzer>
      <analyzer type="query">
        <tokenizer class="solr.WhitespaceTokenizerFactory"/>
        
        <!-- Splits words into subwords and performs optional transformations on 
             subword groups.
             - preserveOriginal="1" in order to preserve the original word 
        -->
        <filter class="solr.WordDelimiterFilterFactory" 
                generateWordParts="1" 
                generateNumberParts="0" 
                catenateWords="0" 
                catenateNumbers="0" 
                catenateAll="0" 
                splitOnCaseChange="1" 
                preserveOriginal="1"/>
        
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
                
        <!-- Filters out those tokens *not* having length min through max 
             inclusive. -->
        <filter class="solr.LengthFilterFactory" min="2" max="256"/>
        
      </analyzer>
    </fieldType>

    <!--
      The SIREn field type:
      Use the TupleAnalyzer 
      - Parse n-triple data 
      - Tokenise Literals but also URIs
      
      Field norms are not useful for SIREn fields. Set omitNorms to true reduces
      memory consumption, and improve ranking.
    -->
    <fieldType name="siren" class="solr.TextField" omitNorms="true">
      <analyzer type="index">

        <tokenizer class="org.sindice.siren.solr.analysis.TupleTokenizerFactory"/>
				
				<filter class="solr.StandardFilterFactory"/>
        
        <!-- Token removal. By default, filter bnode, dot, datatype and language
             tag token.
             Add bnode=0 to not filter blank node tokens. -->
        <filter class="org.sindice.siren.solr.analysis.TokenTypeFilterFactory"/>
        
        <!-- Extract and tokenise localname of URIs.
             Add maxLength=n to not tokenise localname with more than n 
             characters. By default, maxLength=64. -->
        <filter class="org.sindice.siren.solr.analysis.URILocalnameFilterFactory"/>

        <!-- Remove trailing slash of URIs. -->       
        <filter class="org.sindice.siren.solr.analysis.URITrailingSlashFilterFactory"/>
        
        <!-- Change to lowercase text -->
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <!-- Case insensitive stop word removal.
          add enablePositionIncrements=true in both the index and query
          analyzers to leave a 'gap' for more accurate phrase queries. -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
        
        <!-- Filters out those tokens *not* having length min through max 
             inclusive. -->
        <filter class="solr.LengthFilterFactory" min="2" max="256"/>
        
        <!-- Filter converting tuple and cell information in the term payload. -->
        <filter class="org.sindice.siren.solr.analysis.SirenPayloadFilterFactory"/>
        
      </analyzer>
      <analyzer type="query">
        <tokenizer class="org.sindice.siren.solr.analysis.NTripleQueryTokenizerFactory"/>

        <!-- Remove trailing slash of URIs. -->       
        <filter class="org.sindice.siren.solr.analysis.URITrailingSlashFilterFactory"/>
        
        <!-- Change to lowercase text -->
        <filter class="solr.LowerCaseFilterFactory"/>
        
        <!-- Case insensitive stop word removal.
          add enablePositionIncrements=true in both the index and query
          analyzers to leave a 'gap' for more accurate phrase queries. -->
        <filter class="solr.StopFilterFactory"
                ignoreCase="true"
                words="stopwords.txt"
                enablePositionIncrements="true"
                />
      </analyzer>
    </fieldType>
    
 </types>


 <fields>

   <!-- Data attributes -->

    <!-- The ID (URL) of the document 
         Use the 'string' field type (no tokenisation)
    -->
    <field name="id" type="string" indexed="true" stored="true" required="true"/>
   
    <!-- The URL of the document 
         Use the 'text' field type in order to be tokenised
    -->
    <field name="url" type="uri" indexed="true" stored="true" required="true"/>
    
    <!-- Explicit data: original triples -->
    <field name="explicit_content" type="siren" indexed="true" stored="false" multiValued="false"/>
    
    <!-- Implicit data: infered triples -->
    <field name="implicit_content" type="siren" indexed="true" stored="false" multiValued="false"/>
    
   <!-- facet attributes -->
   
    <!-- List of ontologies imported 
         Use the 'text' field type in order to be tokenised
    -->
    <field name="ontology" type="uri" indexed="true" stored="true" multiValued="true"/>
    
    <!-- List of classes used in the document 
         Use the 'text' field type in order to be tokenised
    -->
    <field name="class" type="uri" indexed="true" stored="true" multiValued="true"/>
    
    <!-- Domain of the document 
         Use the 'string' field type (no tokenisation)
    -->
    <field name="domain" type="string" indexed="true" stored="true" multiValued="true"/>
   
    <!-- The source of the data: dump, crawl, sparql, etc... 
         Use the 'string' field type (no tokenisation)
    -->
    <field name="data_source" type="string" indexed="true" stored="true"/>
   
    <!-- The original format of the data: rdf, microformats, ... 
         Use the 'string' field type (no tokenisation)
    -->
    <field name="format" type="string" indexed="true" stored="true" multiValued="true"/>
   
    <!-- If the source of the data is a dump or a sparql endpoint, the URI of the dataset  
         Use the 'string' field type (no tokenisation)
    -->
    <field name="dataset_uri" type="uri" indexed="true" stored="true"/>

   <!-- Ranking attributes -->
   
    <!-- The labels of the document 
         Use the 'text' field type in order to be tokenised
    -->
    <field name="label" type="text" indexed="true" stored="true" multiValued="true"/>
    
    <!-- The page rank of the document -->
    <field name="pagerank" type="float" indexed="false" stored="true" default="1"/>

   <!-- Metadata attributes -->
   
    <!-- Size: number of triples -->
    <field name="size" type="int" indexed="false" stored="true"/>
    <field name="implicit_content_size" type="int" indexed="false" stored="true"/>
  
    <!-- Length: number of bytes -->
    <field name="length" type="int" indexed="false" stored="true"/>
    <field name="implicit_content_length" type="int" indexed="false" stored="true"/>
    
    <!-- Timestamp 
         Use the tdate field type for faster range queries
    -->
    <field name="timestamp" type="tdate" indexed="true" stored="true" default="NOW/DAY"/>

   
 </fields>

 <!-- Field to use to determine and enforce document uniqueness. 
      Unless this field is marked with required="false", it will be a required field
   -->
 <uniqueKey>id</uniqueKey>

 <!-- field for the QueryParser to use when an explicit fieldname is absent -->
 <defaultSearchField>explicit_content</defaultSearchField>

 <!-- SolrQueryParser configuration: defaultOperator="AND|OR" -->
 <solrQueryParser defaultOperator="AND"/>

 <!-- copyField commands copy one field to another at the time a document
      is added to the index.  It's used either to index the same field differently,
      or to add multiple fields to the same field for easier/faster searching.  -->

 <copyField source="url" dest="id"/>

</schema>
